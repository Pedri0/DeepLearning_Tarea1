{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos las librerias utiles\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1: Red de unidades de umbral lineal\n",
    "\n",
    "Programa y evalua una red de neuronas con funciones de activacion escalon unitario que aproxime la operacion XNOR dada por:\n",
    "\n",
    "\n",
    "| $x_1$ | $x_2$ | $y$ |\n",
    "| :- | :- | :- |\n",
    "| 0 | 0 | 1 \n",
    "| 0 | 1 | 0\n",
    "| 1 | 0 | 0\n",
    "| 1 | 1 | 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de activación escalon está dada por \n",
    "\n",
    "$\\phi=   \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      1 & x> 0 \\\\\n",
    "      0 & otro caso \\\\\n",
    "\\end{array} \n",
    "\\right.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creamos una funcion en python para representar la funcion de activacion\n",
    "#escalon\n",
    "def escalon(z):\n",
    "    if z > 0.0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La operación llevada acabo por la neurona artificial es una suma pesada evaluada en la funcion de activación $\\phi$. La suma pesada consiste en multiplicar cada entrada por su correspondiente peso y sumar el sesgo, es decir,\n",
    "\n",
    "$z= w_1 \\cdot x_1 + w_2 \\cdot x_2 + ... + w_n \\cdot x_n + b$\n",
    "\n",
    "o bien,\n",
    "\n",
    "$z = W^{T}\\vec{x} + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un XNOR puede simplificarse de la siguiente manera:\n",
    "\n",
    "$$\\overline{x_1 \\oplus x_2} = (x_1 \\land x_2) \\lor (\\overline{x_1} \\land \\overline{x_2})$$\n",
    "\n",
    "Probar que la expresion booleana anterior se cumple para los $x_1$ y $x_2$ de la tabla es trivial por lo que se deja al lector.\n",
    "\n",
    "La expresion booleana deja en evidencia que nuestra red nueronal o perceptron debe contener tres neuronas \"internas\" que deben cumplir las operaciones de AND, NOR y OR. Debido a esto es necesario vectorizar nuestra funcion de activacion escalon ya que ahora esta funcion comera vectores, esto se hace con la funcion de numpy np.vectorize(function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definimos el perceptron multicapa\n",
    "def perceptronMulticapa(vector_x, W1, b1, W2, b2):\n",
    "    #realizamos la funcion escalon vectorizado\n",
    "    escalonVectorizado = np.vectorize(escalon)\n",
    "    #definimos/obtenemos la salida de la primera capa oculta\n",
    "    capa1 = escalonVectorizado(np.dot(W1.T , vector_x) + b1)\n",
    "    #definimos/obtenemos la salida de la ultima capa\n",
    "    capa2 = escalonVectorizado(np.dot(W2.T, capa1) + b2)\n",
    "    #regresa el valor de la ultima capa\n",
    "    return capa2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probemos con las siguientes matrices de pesos y sesgos para reproducir la compuerta XNOR:\n",
    "\n",
    "$$W_1 = \\begin{pmatrix}\n",
    "  1 & -1\\\\ \n",
    "  1 & -1\n",
    "\\end{pmatrix}$$  \n",
    "$$\\vec{b_1} = (-1,1)$$\n",
    "$$W_2 =(2,2)$$\n",
    "$$b_2 = -1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "x_1 \tx_2 \ty\ty_hat\n",
      "-----------------------------\n",
      "0.0\t0.0\t1.0\t1.0\n",
      "0.0\t1.0\t0.0\t0.0\n",
      "1.0\t0.0\t0.0\t0.0\n",
      "1.0\t1.0\t1.0\t1.0\n"
     ]
    }
   ],
   "source": [
    "#definimos el vector de entradas x1 y x2\n",
    "X = np.array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])\n",
    "#definimos las salidas del xnor\n",
    "y_xnor = np.array([1., 0., 0., 1.])\n",
    "#definimos la matriz de pesos W1 y el sesgo b1\n",
    "W1 = np.array([[1, -1], [1, -1]])\n",
    "b1 = np.array([-1, 1])\n",
    "#definimos la matriz de pesos W2 y el sesgo b2\n",
    "W2 = np.array([[2], [2]])\n",
    "b2 = np.array([-1])\n",
    "#Imprimimos en forma de tabla todos los valores de x_j, y y la prediccion\n",
    "print('-----------------------------')\n",
    "print('x_1 \\tx_2 \\ty\\ty_hat')\n",
    "print('-----------------------------')\n",
    "#iteramos sobre todos los valores de X\n",
    "for i in range(X.shape[0]):\n",
    "    #realizamos la prediccion haciendo uso del perceptronMulticapa\n",
    "    y_hat = perceptronMulticapa(X[i], W1, b1, W2, b2)\n",
    "    print('{0}\\t{1}\\t{2}\\t{3}'.format(X[i, 0], X[i, 1], y_xnor[i], y_hat[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora hay que definir la retropropagacion y la funcion de entrenamiento para que el perceptronMulticapa aprenda por si solo unos pesos que modelen la XNOR.\n",
    "Vamos a definir una red densa con 2 neuronas de entrada y una sola capa oculta con 15 neuronas con funcion de activacion sigmoide y una neurona de salida con la misma funcion de activacion. La funcion sigmoide esta definida como:\n",
    "\n",
    "$$\\sigma (z) = \\frac{1}{1+exp(-z)}$$\n",
    "\n",
    "Cuya derivada es:\n",
    "\n",
    "$$\\frac{d}{dz} \\sigma (z) = \\sigma (z) (1-\\sigma (z))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creamos la funcion sigmoide\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "#creamos la derivada de la funcion sigmoide\n",
    "def sigmoidDerivative(w):\n",
    "    return np.multiply(sigmoid(w), (1-sigmoid(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que la compuerta XNOR recibe dos entradas y escupe dos salidas podemos pensar esta XNOR como un problema de clasificacion binaria. Por esta razon tenemos que usar la funcion de perdida entopia binaria cruzada que se define como:\n",
    "\n",
    "$$BinaryCrossEntropy(\\vec{y},\\hat{\\vec{y}}) = -\\sum^{N}_{i=1} \\left(y^{(i)}log\\hat{y}^{(i)} + (1-y^{(i)})log(1-\\hat{y}^{(i)})  \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creamos la funcion de perdida entropia cruzada binaria\n",
    "def BCrossEnt(y,p):\n",
    "    p[p == 0] = np.nextafter(0., 1.)\n",
    "    p[p == 1] = np.nextafter(1., 0.)\n",
    "    return -(np.log(p[y == 1]).sum() + np.log(1 - p[y == 0]).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una metrica para medir la calidad de las predicciones se encuentra en la exactitud definida como:\n",
    "\n",
    "$$Exactitud = \\frac{pred correctas}{total de datos}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definimos la funcion de exactitud\n",
    "def exactitud(y, prediccion):\n",
    "    return (y == prediccion).mean() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La propagacion hacia adelante depende de la red densa. Como hay una capa oculta y una de salida vamos a tener dos matrices de pesos $W_{1,2}$ y dos sesgos $b_{1,2}$ una para cada una de las capas. Para este caso la propagacion hacia adelante se define como:\n",
    "\n",
    "$$\\vec{a}^{(1)} = \\vec{x}^{(i)}$$\n",
    "$$\\vec{z}^{(2)} = W^{(1)T} \\cdot \\vec{a}^{(1)} + \\vec{b}^{(1)}$$\n",
    "$$\\vec{a}^{(2)} = \\phi (\\vec{z}^{2})$$\n",
    "$$\\vec{z}^{(3)} = W^{(2)T} \\cdot \\vec{a}^{(2)} + \\vec{b}^{(2)}$$\n",
    "$$\\vec{a}^{(3)} = \\phi (\\vec{z}^{3})$$\n",
    "$$\\hat{y}^{(i)} = a{(3)}$$\n",
    "\n",
    "donde $\\phi$ es la funcion de activacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definimos la funcion de propagacion\n",
    "def propagacion(x, w1, b1, w2, b2):\n",
    "    #calculamos z2\n",
    "    z2 = (w1.T @ x[:, np.newaxis]) + b1\n",
    "    #z2 = np.dot(w1.T,x[:, np.newaxis]) + b1\n",
    "    #calculamos a2\n",
    "    a2 = sigmoid(z2)\n",
    "    #calculamos z3\n",
    "    #z3 = (w2.T @ a2) + b2\n",
    "    z3 = np.dot(w2.T, a2) + b2\n",
    "    #hacemos la prediccion (calculamos a3)\n",
    "    prediccion = sigmoid(z3)\n",
    "    #regresamos los valores que acabamos de calcular\n",
    "    return z2, a2, z3, prediccion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entrenar vamos a usar el descenso del gradiente, que deriva la funcion de perdida y propaga hacia atras o retroporpaga con respecto a los pesos y sesgos en cada una de estas capas. El algoritmo de retropropagacion nos dice que: Debemos calcular las derivadas parciales de la perdida con respecto a cada peso y sesgo capa por capa, comenzando por la capa de salida y propogandolas hacia atras para calcular las de la capa anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definimos la funcion de retropropagacion\n",
    "def retropropagacion(x, y, learningRate = 0.1, epochs = 50, neuronasOcultas=10):\n",
    "    ejemplos = x.shape[0]\n",
    "    entradas = x.shape[1]\n",
    "    #inicializacion de las matrices W y sesgos b\n",
    "    W1 = np.sqrt(1.0 / entradas) * np.random.randn(entradas, neuronasOcultas)\n",
    "    b1 = np.zeros((neuronasOcultas, 1))\n",
    "    W2 = np.sqrt(1.0 / neuronasOcultas) * np.random.randn(neuronasOcultas, 1)\n",
    "    b2 = np.zeros((1, 1))\n",
    "    #definimos el vector de perdidas, que en cada epoca se va a actualizar\n",
    "    perdidas = np.zeros((epochs))\n",
    "    #definimos el vector dew exactitudes que en cada epoca se ca a actualizar\n",
    "    exactitudes = np.zeros((epochs))\n",
    "    #definimos el vector de predicciones que corresponde al numero de salidas\n",
    "    predicciones = np.zeros((y.shape))\n",
    "    #iteramos sobre las epocas\n",
    "    for epoch in range(epochs):\n",
    "        #iteramos sobre todos los ejemplos\n",
    "        for ejemplo in range(ejemplos):\n",
    "            #calculamos los vectores resultantes de la propagacion\n",
    "            z2, a2, z3, prediccion = propagacion(x[ejemplo], W1, b1, W2, b2)\n",
    "            #calculamos el gradiente de la ultima capa\n",
    "            gradCapa2 = (prediccion - y[ejemplo]) * sigmoidDerivative(z3)\n",
    "            #actualizamos los pesos de W2 y b2\n",
    "            W2 = W2 - learningRate * np.outer(a2, gradCapa2)\n",
    "            b2 = b2 - learningRate * gradCapa2\n",
    "            #calculamos el gradiente de la capa oculta\n",
    "            gradCapa1 = (W2 @ gradCapa2) * sigmoidDerivative(z2)\n",
    "            #actualizamos los pesos de W1 y b1\n",
    "            W1 = W1 - learningRate * np.outer(x[ejemplo], gradCapa1)\n",
    "            b1 = b1 - learningRate * gradCapa1\n",
    "            #asignamos nuestra prediccion\n",
    "            predicciones[ejemplo] = prediccion\n",
    "        #calculamos la perdida para la epoca actual\n",
    "        perdidas[epoch] = BCrossEnt(y,predicciones)\n",
    "        #calculamos las exactitudes para la epoca actual\n",
    "        exactitudes[epoch] = exactitud(y, np.round(predicciones))\n",
    "        #imprimimos la perdida y la exactitud de cada epoca\n",
    "        print('Epoch {0}: Perdida = {1} Exactitud = {2}'.format(epoch,perdidas[epoch],exactitudes[epoch]))\n",
    "    return W1, W2, b1, b2, perdidas, exactitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Perdida = 3.7046778255065917 Exactitud = 25.0\n",
      "Epoch 1: Perdida = 3.6955582260888757 Exactitud = 25.0\n",
      "Epoch 2: Perdida = 3.715727860843767 Exactitud = 50.0\n",
      "Epoch 3: Perdida = 3.7287606128438737 Exactitud = 50.0\n",
      "Epoch 4: Perdida = 3.7358709254535833 Exactitud = 50.0\n",
      "Epoch 5: Perdida = 3.740094742634228 Exactitud = 50.0\n",
      "Epoch 6: Perdida = 3.743043131915277 Exactitud = 50.0\n",
      "Epoch 7: Perdida = 3.74543642483728 Exactitud = 50.0\n",
      "Epoch 8: Perdida = 3.747580442525849 Exactitud = 50.0\n",
      "Epoch 9: Perdida = 3.749602020348803 Exactitud = 50.0\n",
      "Epoch 10: Perdida = 3.7515527549902874 Exactitud = 50.0\n",
      "Epoch 11: Perdida = 3.7534530009745932 Exactitud = 50.0\n",
      "Epoch 12: Perdida = 3.7553102562481073 Exactitud = 50.0\n",
      "Epoch 13: Perdida = 3.757126800313455 Exactitud = 50.0\n",
      "Epoch 14: Perdida = 3.7589028632661154 Exactitud = 50.0\n",
      "Epoch 15: Perdida = 3.7606379450077014 Exactitud = 50.0\n",
      "Epoch 16: Perdida = 3.762331371040891 Exactitud = 50.0\n",
      "Epoch 17: Perdida = 3.7639825339067756 Exactitud = 50.0\n",
      "Epoch 18: Perdida = 3.7655910052992025 Exactitud = 50.0\n",
      "Epoch 19: Perdida = 3.767156594731712 Exactitud = 50.0\n",
      "Epoch 20: Perdida = 3.768679385540085 Exactitud = 50.0\n",
      "Epoch 21: Perdida = 3.770159760361036 Exactitud = 50.0\n",
      "Epoch 22: Perdida = 3.771598420486975 Exactitud = 50.0\n",
      "Epoch 23: Perdida = 3.7729964002450824 Exactitud = 50.0\n",
      "Epoch 24: Perdida = 3.774355076149033 Exactitud = 50.0\n",
      "Epoch 25: Perdida = 3.7756761699452293 Exactitud = 50.0\n",
      "Epoch 26: Perdida = 3.7769617443870134 Exactitud = 50.0\n",
      "Epoch 27: Perdida = 3.778214190449913 Exactitud = 50.0\n",
      "Epoch 28: Perdida = 3.7794362046875283 Exactitud = 50.0\n",
      "Epoch 29: Perdida = 3.7806307555045024 Exactitud = 50.0\n",
      "Epoch 30: Perdida = 3.7818010372901414 Exactitud = 50.0\n",
      "Epoch 31: Perdida = 3.782950411618196 Exactitud = 50.0\n",
      "Epoch 32: Perdida = 3.7840823350776907 Exactitud = 50.0\n",
      "Epoch 33: Perdida = 3.7852002737537527 Exactitud = 50.0\n",
      "Epoch 34: Perdida = 3.7863076049152893 Exactitud = 50.0\n",
      "Epoch 35: Perdida = 3.7874075070681763 Exactitud = 50.0\n",
      "Epoch 36: Perdida = 3.7885028401684107 Exactitud = 50.0\n",
      "Epoch 37: Perdida = 3.789596018420791 Exactitud = 50.0\n",
      "Epoch 38: Perdida = 3.790688878669922 Exactitud = 50.0\n",
      "Epoch 39: Perdida = 3.7917825478734315 Exactitud = 50.0\n",
      "Epoch 40: Perdida = 3.7928773134862706 Exactitud = 50.0\n",
      "Epoch 41: Perdida = 3.793972500741275 Exactitud = 50.0\n",
      "Epoch 42: Perdida = 3.795066360759488 Exactitud = 50.0\n",
      "Epoch 43: Perdida = 3.7961559731561403 Exactitud = 50.0\n",
      "Epoch 44: Perdida = 3.7972371663369473 Exactitud = 50.0\n",
      "Epoch 45: Perdida = 3.798304458036668 Exactitud = 50.0\n",
      "Epoch 46: Perdida = 3.799351017886933 Exactitud = 50.0\n",
      "Epoch 47: Perdida = 3.8003686529738543 Exactitud = 50.0\n",
      "Epoch 48: Perdida = 3.8013478165225765 Exactitud = 50.0\n",
      "Epoch 49: Perdida = 3.8022776390863844 Exactitud = 50.0\n",
      "Epoch 50: Perdida = 3.8031459809715678 Exactitud = 50.0\n",
      "Epoch 51: Perdida = 3.8039395041287545 Exactitud = 50.0\n",
      "Epoch 52: Perdida = 3.804643761400359 Exactitud = 50.0\n",
      "Epoch 53: Perdida = 3.805243300827565 Exactitud = 50.0\n",
      "Epoch 54: Perdida = 3.8057217826687735 Exactitud = 50.0\n",
      "Epoch 55: Perdida = 3.8060621068347795 Exactitud = 50.0\n",
      "Epoch 56: Perdida = 3.8062465485701718 Exactitud = 50.0\n",
      "Epoch 57: Perdida = 3.806256900372593 Exactitud = 50.0\n",
      "Epoch 58: Perdida = 3.806074618314379 Exactitud = 50.0\n",
      "Epoch 59: Perdida = 3.805680971095245 Exactitud = 50.0\n",
      "Epoch 60: Perdida = 3.805057190299145 Exactitud = 50.0\n",
      "Epoch 61: Perdida = 3.804184620450018 Exactitud = 50.0\n",
      "Epoch 62: Perdida = 3.8030448675623383 Exactitud = 50.0\n",
      "Epoch 63: Perdida = 3.801619944969169 Exactitud = 75.0\n",
      "Epoch 64: Perdida = 3.79989241528994 Exactitud = 75.0\n",
      "Epoch 65: Perdida = 3.7978455274788807 Exactitud = 75.0\n",
      "Epoch 66: Perdida = 3.7954633479774382 Exactitud = 75.0\n",
      "Epoch 67: Perdida = 3.7927308850821237 Exactitud = 75.0\n",
      "Epoch 68: Perdida = 3.7896342057325305 Exactitud = 75.0\n",
      "Epoch 69: Perdida = 3.786160544020232 Exactitud = 75.0\n",
      "Epoch 70: Perdida = 3.7822984008139118 Exactitud = 75.0\n",
      "Epoch 71: Perdida = 3.778037633985065 Exactitud = 75.0\n",
      "Epoch 72: Perdida = 3.7733695387976387 Exactitud = 75.0\n",
      "Epoch 73: Perdida = 3.7682869180907455 Exactitud = 75.0\n",
      "Epoch 74: Perdida = 3.762784141934192 Exactitud = 75.0\n",
      "Epoch 75: Perdida = 3.7568571964715165 Exactitud = 75.0\n",
      "Epoch 76: Perdida = 3.750503721685849 Exactitud = 75.0\n",
      "Epoch 77: Perdida = 3.743723037833087 Exactitud = 75.0\n",
      "Epoch 78: Perdida = 3.736516160288783 Exactitud = 75.0\n",
      "Epoch 79: Perdida = 3.7288858025550162 Exactitud = 75.0\n",
      "Epoch 80: Perdida = 3.7208363671774163 Exactitud = 75.0\n",
      "Epoch 81: Perdida = 3.712373924336612 Exactitud = 75.0\n",
      "Epoch 82: Perdida = 3.703506177908964 Exactitud = 75.0\n",
      "Epoch 83: Perdida = 3.6942424188441274 Exactitud = 75.0\n",
      "Epoch 84: Perdida = 3.6845934657867 Exactitud = 75.0\n",
      "Epoch 85: Perdida = 3.6745715929792633 Exactitud = 75.0\n",
      "Epoch 86: Perdida = 3.66419044562656 Exactitud = 75.0\n",
      "Epoch 87: Perdida = 3.653464943075065 Exactitud = 75.0\n",
      "Epoch 88: Perdida = 3.6424111703663566 Exactitud = 75.0\n",
      "Epoch 89: Perdida = 3.6310462589516916 Exactitud = 75.0\n",
      "Epoch 90: Perdida = 3.6193882576016234 Exactitud = 75.0\n",
      "Epoch 91: Perdida = 3.6074559947986486 Exactitud = 75.0\n",
      "Epoch 92: Perdida = 3.5952689341512727 Exactitud = 75.0\n",
      "Epoch 93: Perdida = 3.5828470246013495 Exactitud = 75.0\n",
      "Epoch 94: Perdida = 3.5702105473995163 Exactitud = 75.0\n",
      "Epoch 95: Perdida = 3.5573799619827247 Exactitud = 75.0\n",
      "Epoch 96: Perdida = 3.5443757529911757 Exactitud = 75.0\n",
      "Epoch 97: Perdida = 3.531218280699893 Exactitud = 75.0\n",
      "Epoch 98: Perdida = 3.5179276371062294 Exactitud = 75.0\n",
      "Epoch 99: Perdida = 3.504523509806629 Exactitud = 75.0\n",
      "Epoch 100: Perdida = 3.491025055615671 Exactitud = 75.0\n",
      "Epoch 101: Perdida = 3.477450785634443 Exactitud = 75.0\n",
      "Epoch 102: Perdida = 3.463818463173772 Exactitud = 75.0\n",
      "Epoch 103: Perdida = 3.450145015594534 Exactitud = 75.0\n",
      "Epoch 104: Perdida = 3.436446460758079 Exactitud = 75.0\n",
      "Epoch 105: Perdida = 3.422737848401699 Exactitud = 75.0\n",
      "Epoch 106: Perdida = 3.4090332163839014 Exactitud = 75.0\n",
      "Epoch 107: Perdida = 3.3953455613975887 Exactitud = 75.0\n",
      "Epoch 108: Perdida = 3.381686823439257 Exactitud = 75.0\n",
      "Epoch 109: Perdida = 3.36806788305934 Exactitud = 75.0\n",
      "Epoch 110: Perdida = 3.3544985702096044 Exactitud = 75.0\n",
      "Epoch 111: Perdida = 3.3409876833513965 Exactitud = 75.0\n",
      "Epoch 112: Perdida = 3.3275430173935914 Exactitud = 75.0\n",
      "Epoch 113: Perdida = 3.3141713989884476 Exactitud = 75.0\n",
      "Epoch 114: Perdida = 3.3008787277219946 Exactitud = 75.0\n",
      "Epoch 115: Perdida = 3.287670021786262 Exactitud = 75.0\n",
      "Epoch 116: Perdida = 3.274549466805764 Exactitud = 75.0\n",
      "Epoch 117: Perdida = 3.2615204666018753 Exactitud = 75.0\n",
      "Epoch 118: Perdida = 3.248585694808092 Exactitud = 75.0\n",
      "Epoch 119: Perdida = 3.2357471463890315 Exactitud = 75.0\n",
      "Epoch 120: Perdida = 3.2230061882598267 Exactitud = 75.0\n",
      "Epoch 121: Perdida = 3.210363608344651 Exactitud = 75.0\n",
      "Epoch 122: Perdida = 3.1978196625490214 Exactitud = 75.0\n",
      "Epoch 123: Perdida = 3.1853741192468448 Exactitud = 75.0\n",
      "Epoch 124: Perdida = 3.173026300997706 Exactitud = 75.0\n",
      "Epoch 125: Perdida = 3.1607751233110686 Exactitud = 75.0\n",
      "Epoch 126: Perdida = 3.148619130361447 Exactitud = 75.0\n",
      "Epoch 127: Perdida = 3.136556527632196 Exactitud = 75.0\n",
      "Epoch 128: Perdida = 3.1245852115257584 Exactitud = 75.0\n",
      "Epoch 129: Perdida = 3.1127027960262117 Exactitud = 75.0\n",
      "Epoch 130: Perdida = 3.100906636536469 Exactitud = 75.0\n",
      "Epoch 131: Perdida = 3.089193851039213 Exactitud = 75.0\n",
      "Epoch 132: Perdida = 3.0775613387485086 Exactitud = 75.0\n",
      "Epoch 133: Perdida = 3.066005796429639 Exactitud = 75.0\n",
      "Epoch 134: Perdida = 3.0545237325690904 Exactitud = 75.0\n",
      "Epoch 135: Perdida = 3.0431114795759973 Exactitud = 75.0\n",
      "Epoch 136: Perdida = 3.0317652041918155 Exactitud = 75.0\n",
      "Epoch 137: Perdida = 3.0204809162773922 Exactitud = 75.0\n",
      "Epoch 138: Perdida = 3.0092544761366935 Exactitud = 75.0\n",
      "Epoch 139: Perdida = 2.998081600525035 Exactitud = 75.0\n",
      "Epoch 140: Perdida = 2.98695786747718 Exactitud = 75.0\n",
      "Epoch 141: Perdida = 2.975878720077568 Exactitud = 75.0\n",
      "Epoch 142: Perdida = 2.9648394692817233 Exactitud = 75.0\n",
      "Epoch 143: Perdida = 2.9538352958845713 Exactitud = 75.0\n",
      "Epoch 144: Perdida = 2.942861251718514 Exactitud = 75.0\n",
      "Epoch 145: Perdida = 2.931912260151426 Exactitud = 75.0\n",
      "Epoch 146: Perdida = 2.920983115942721 Exactitud = 75.0\n",
      "Epoch 147: Perdida = 2.9100684845040417 Exactitud = 75.0\n",
      "Epoch 148: Perdida = 2.899162900600146 Exactitud = 75.0\n",
      "Epoch 149: Perdida = 2.8882607665151676 Exactitud = 75.0\n",
      "Epoch 150: Perdida = 2.8773563496995416 Exactitud = 75.0\n",
      "Epoch 151: Perdida = 2.8664437799035847 Exactitud = 75.0\n",
      "Epoch 152: Perdida = 2.8555170457949197 Exactitud = 75.0\n",
      "Epoch 153: Perdida = 2.844569991048661 Exactitud = 75.0\n",
      "Epoch 154: Perdida = 2.833596309891453 Exactitud = 75.0\n",
      "Epoch 155: Perdida = 2.822589542073291 Exactitud = 75.0\n",
      "Epoch 156: Perdida = 2.8115430672342967 Exactitud = 75.0\n",
      "Epoch 157: Perdida = 2.800450098627686 Exactitud = 75.0\n",
      "Epoch 158: Perdida = 2.7893036761548755 Exactitud = 75.0\n",
      "Epoch 159: Perdida = 2.7780966586644045 Exactitud = 75.0\n",
      "Epoch 160: Perdida = 2.7668217154632067 Exactitud = 75.0\n",
      "Epoch 161: Perdida = 2.755471316987115 Exactitud = 75.0\n",
      "Epoch 162: Perdida = 2.744037724577658 Exactitud = 75.0\n",
      "Epoch 163: Perdida = 2.732512979314662 Exactitud = 75.0\n",
      "Epoch 164: Perdida = 2.720888889859639 Exactitud = 75.0\n",
      "Epoch 165: Perdida = 2.7091570192738974 Exactitud = 75.0\n",
      "Epoch 166: Perdida = 2.6973086707888747 Exactitud = 75.0\n",
      "Epoch 167: Perdida = 2.685334872525214 Exactitud = 75.0\n",
      "Epoch 168: Perdida = 2.6732263611829428 Exactitud = 75.0\n",
      "Epoch 169: Perdida = 2.6609735647591917 Exactitud = 75.0\n",
      "Epoch 170: Perdida = 2.6485665843937705 Exactitud = 75.0\n",
      "Epoch 171: Perdida = 2.6359951754987385 Exactitud = 75.0\n",
      "Epoch 172: Perdida = 2.6232487283976544 Exactitud = 75.0\n",
      "Epoch 173: Perdida = 2.6103162487861438 Exactitud = 75.0\n",
      "Epoch 174: Perdida = 2.59718633842974 Exactitud = 75.0\n",
      "Epoch 175: Perdida = 2.58384717664035 Exactitud = 75.0\n",
      "Epoch 176: Perdida = 2.5702865032210243 Exactitud = 75.0\n",
      "Epoch 177: Perdida = 2.5564916037414953 Exactitud = 75.0\n",
      "Epoch 178: Perdida = 2.542449298204798 Exactitud = 75.0\n",
      "Epoch 179: Perdida = 2.528145934386857 Exactitud = 75.0\n",
      "Epoch 180: Perdida = 2.5135673873728313 Exactitud = 75.0\n",
      "Epoch 181: Perdida = 2.4986990670691176 Exactitud = 75.0\n",
      "Epoch 182: Perdida = 2.4835259357269273 Exactitud = 75.0\n",
      "Epoch 183: Perdida = 2.468032537754951 Exactitud = 75.0\n",
      "Epoch 184: Perdida = 2.452203044300732 Exactitud = 75.0\n",
      "Epoch 185: Perdida = 2.4360213152111645 Exactitud = 75.0\n",
      "Epoch 186: Perdida = 2.419470981002068 Exactitud = 75.0\n",
      "Epoch 187: Perdida = 2.4025355473280348 Exactitud = 75.0\n",
      "Epoch 188: Perdida = 2.385198524095178 Exactitud = 75.0\n",
      "Epoch 189: Perdida = 2.3674435807499457 Exactitud = 75.0\n",
      "Epoch 190: Perdida = 2.349254728363742 Exactitud = 75.0\n",
      "Epoch 191: Perdida = 2.3306165278922997 Exactitud = 75.0\n",
      "Epoch 192: Perdida = 2.311514322429768 Exactitud = 75.0\n",
      "Epoch 193: Perdida = 2.2919344894576836 Exactitud = 75.0\n",
      "Epoch 194: Perdida = 2.2718647071282274 Exactitud = 75.0\n",
      "Epoch 195: Perdida = 2.2512942267099243 Exactitud = 75.0\n",
      "Epoch 196: Perdida = 2.230214141722828 Exactitud = 75.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 197: Perdida = 2.2086176433123197 Exactitud = 75.0\n",
      "Epoch 198: Perdida = 2.186500251384944 Exactitud = 75.0\n",
      "Epoch 199: Perdida = 2.163860012242083 Exactitud = 75.0\n",
      "Epoch 200: Perdida = 2.140697656066741 Exactitud = 75.0\n",
      "Epoch 201: Perdida = 2.117016711614633 Exactitud = 75.0\n",
      "Epoch 202: Perdida = 2.0928235805369892 Exactitud = 75.0\n",
      "Epoch 203: Perdida = 2.068127579323245 Exactitud = 75.0\n",
      "Epoch 204: Perdida = 2.042940962029382 Exactitud = 75.0\n",
      "Epoch 205: Perdida = 2.01727894071165 Exactitud = 75.0\n",
      "Epoch 206: Perdida = 1.9911597217759929 Exactitud = 75.0\n",
      "Epoch 207: Perdida = 1.964604574466049 Exactitud = 75.0\n",
      "Epoch 208: Perdida = 1.937637942089001 Exactitud = 75.0\n",
      "Epoch 209: Perdida = 1.9102875975999076 Exactitud = 75.0\n",
      "Epoch 210: Perdida = 1.8825848338216113 Exactitud = 75.0\n",
      "Epoch 211: Perdida = 1.8545646664886466 Exactitud = 75.0\n",
      "Epoch 212: Perdida = 1.8262660174787297 Exactitud = 100.0\n",
      "Epoch 213: Perdida = 1.79773183807138 Exactitud = 100.0\n",
      "Epoch 214: Perdida = 1.7690091295081922 Exactitud = 100.0\n",
      "Epoch 215: Perdida = 1.7401488214462484 Exactitud = 100.0\n",
      "Epoch 216: Perdida = 1.7112054780679833 Exactitud = 100.0\n",
      "Epoch 217: Perdida = 1.6822368156380207 Exactitud = 100.0\n",
      "Epoch 218: Perdida = 1.653303032384978 Exactitud = 100.0\n",
      "Epoch 219: Perdida = 1.6244659694692651 Exactitud = 100.0\n",
      "Epoch 220: Perdida = 1.5957881381407806 Exactitud = 100.0\n",
      "Epoch 221: Perdida = 1.5673316609678103 Exactitud = 100.0\n",
      "Epoch 222: Perdida = 1.5391571828085642 Exactitud = 100.0\n",
      "Epoch 223: Perdida = 1.5113228093429105 Exactitud = 100.0\n",
      "Epoch 224: Perdida = 1.4838831276125748 Exactitud = 100.0\n",
      "Epoch 225: Perdida = 1.456888354942949 Exactitud = 100.0\n",
      "Epoch 226: Perdida = 1.4303836511367258 Exactitud = 100.0\n",
      "Epoch 227: Perdida = 1.404408615487947 Exactitud = 100.0\n",
      "Epoch 228: Perdida = 1.3789969765248147 Exactitud = 100.0\n",
      "Epoch 229: Perdida = 1.3541764698179528 Exactitud = 100.0\n",
      "Epoch 230: Perdida = 1.3299688887179113 Exactitud = 100.0\n",
      "Epoch 231: Perdida = 1.3063902851335765 Exactitud = 100.0\n",
      "Epoch 232: Perdida = 1.2834512926452217 Exactitud = 100.0\n",
      "Epoch 233: Perdida = 1.261157542224045 Exactitud = 100.0\n",
      "Epoch 234: Perdida = 1.2395101412071599 Exactitud = 100.0\n",
      "Epoch 235: Perdida = 1.2185061884051152 Exactitud = 100.0\n",
      "Epoch 236: Perdida = 1.1981393016990523 Exactitud = 100.0\n",
      "Epoch 237: Perdida = 1.1784001386486853 Exactitud = 100.0\n",
      "Epoch 238: Perdida = 1.1592768949971015 Exactitud = 100.0\n",
      "Epoch 239: Perdida = 1.1407557701544178 Exactitud = 100.0\n",
      "Epoch 240: Perdida = 1.1228213925181345 Exactitud = 100.0\n",
      "Epoch 241: Perdida = 1.105457200700484 Exactitud = 100.0\n",
      "Epoch 242: Perdida = 1.088645779327425 Exactitud = 100.0\n",
      "Epoch 243: Perdida = 1.0723691500605563 Exactitud = 100.0\n",
      "Epoch 244: Perdida = 1.0566090199254004 Exactitud = 100.0\n",
      "Epoch 245: Perdida = 1.04134698998474 Exactitud = 100.0\n",
      "Epoch 246: Perdida = 1.0265647279599934 Exactitud = 100.0\n",
      "Epoch 247: Perdida = 1.0122441086626617 Exactitud = 100.0\n",
      "Epoch 248: Perdida = 0.9983673261290874 Exactitud = 100.0\n",
      "Epoch 249: Perdida = 0.9849169812221498 Exactitud = 100.0\n",
      "Epoch 250: Perdida = 0.9718761482277342 Exactitud = 100.0\n",
      "Epoch 251: Perdida = 0.9592284236749882 Exactitud = 100.0\n"
     ]
    }
   ],
   "source": [
    "#definimos las entradas\n",
    "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "#definimos las salidas\n",
    "y = np.array([[1, 0, 0, 1]]).T\n",
    "#Para reproducibilidad ponemos una semilla\n",
    "np.random.seed(0)\n",
    "#entrenamos la red\n",
    "W1, W2, b1, b2, perdidas, exactitudes = retropropagacion(x,y,epochs=252,learningRate=1,neuronasOcultas=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para los hiperparametros encontre que:\n",
    "* Si el learning rate es muy alto > 20, entonces la exactitud se queda atascado en un solo valor\n",
    "* Si el learning rate es menor que 1, entonces la perdida disminuye mul lentamente pero la exactitud tiende a incrementar\n",
    "* Si se ponen muchas neuronas > 25 pero <200 con un learning rate de 1 la exactitud tiende a 75\n",
    "* Si se ponen mas de 200 neuronas con  un LR de 1 la exactitud tiende a 50\n",
    "* Con solo tres neuronas son necesarias mas de 252 epocas para que la exactitud supere 75\n",
    "* Para alcanzar 100 de exactitud los hiperparametros deben setearse en 252 epocas, LR de 1 y 15 neuronas ocultas. Seguramente hay otros hiperparametros que llevan a exactitud 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Con los hiperparametros de 252 epocas, LR de 1 y 15 neuronas ocultas se encuentra una exactitud de 100\n",
      "Para dichos hiperparametros tenemos que\n",
      "Matriz de pesos w1\n",
      "[[ 1.86865956  1.03379409  1.21366308  2.86418801  4.50577226 -3.76204478\n",
      "   1.11177628  0.02605481 -0.18062277  2.42840306  0.49062748  1.35240693\n",
      "   0.8369279  -0.53836755  1.68111478]\n",
      " [-0.21410944  1.24523688 -0.01223259  2.32660392 -2.71534477 -3.61744092\n",
      "   0.33682876  1.26907495 -1.45152562  2.45613553 -2.88681072  0.26422565\n",
      "   0.04734949  2.14270185  1.64170684]]\n",
      "############################################################\n",
      "vector de sesgos b1\n",
      "[[0.26358041]\n",
      " [0.8825361 ]\n",
      " [0.50913269]\n",
      " [0.42735846]\n",
      " [1.21846439]\n",
      " [0.59154149]\n",
      " [0.30889284]\n",
      " [0.35355281]\n",
      " [0.64846364]\n",
      " [0.61512247]\n",
      " [0.54865341]\n",
      " [0.58997903]\n",
      " [0.63552943]\n",
      " [0.11821346]\n",
      " [1.02122142]]\n",
      "############################################################\n",
      "Matriz de pesos w2\n",
      "[[ 0.92698966]\n",
      " [-0.17290522]\n",
      " [ 0.44633847]\n",
      " [-2.35860415]\n",
      " [ 4.20195398]\n",
      " [ 3.59973058]\n",
      " [ 0.65309987]\n",
      " [ 1.02393678]\n",
      " [-1.52124386]\n",
      " [-2.0559442 ]\n",
      " [-2.86485395]\n",
      " [ 0.22285439]\n",
      " [ 0.08554478]\n",
      " [ 1.83705784]\n",
      " [-1.03884137]]\n",
      "############################################################\n",
      "vector de sesgos b2\n",
      "[[-0.39468439]]\n",
      "############################################################\n"
     ]
    }
   ],
   "source": [
    "print('Con los hiperparametros de 252 epocas, LR de 1 y 15 neuronas ocultas se encuentra una exactitud de 100')\n",
    "print('Para dichos hiperparametros tenemos que')\n",
    "print('Matriz de pesos w1')\n",
    "print(W1)\n",
    "print('############################################################')\n",
    "print('vector de sesgos b1')\n",
    "print(b1)\n",
    "print('############################################################')\n",
    "print('Matriz de pesos w2')\n",
    "print(W2)\n",
    "print('############################################################')\n",
    "print('vector de sesgos b2')\n",
    "print(b2)\n",
    "print('############################################################')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprobar que realmente los valores son correctos podemos hacer las operaciones manualmente. Pero ya tenemos la funcion de propagacion que lo hace y nos regresa la prediccion, entonces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "x_1 \tx_2 \ty\ty_hat\n",
      "-----------------------------\n",
      "0\t0\t1.0\t1.0\n",
      "0\t1\t0.0\t0.0\n",
      "1\t0\t0.0\t0.0\n",
      "1\t1\t1.0\t1.0\n"
     ]
    }
   ],
   "source": [
    "print('-----------------------------')\n",
    "print('x_1 \\tx_2 \\ty\\ty_hat')\n",
    "print('-----------------------------')\n",
    "for i in range(x.shape[0]):\n",
    "    #calculamos la prediccion usando las W's y las b's con la funcion de propagacion\n",
    "    #los valores de a2, z2 y z3 no se usan por eso tienen el nombre de foo\n",
    "    foo1, foo2, foo3, prediccionDespuesdeEntrenar = propagacion(x[i],W1,b1,W2,b2)\n",
    "    print('{0}\\t{1}\\t{2}\\t{3}'.format(x[i, 0], x[i, 1], y_xnor[i], np.round(prediccionDespuesdeEntrenar[0][0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora Graficamos las perdidas por cada epoca y la exactitud por cada epoca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#reescalamos la exactitud entre 0-1 y no entre 0-100\n",
    "foo = exactitudes/100\n",
    "#ploteamos la perdida y la exactitud a traves de las epocas\n",
    "plt.plot(np.arange(perdidas.size), perdidas, '.', label='Perdida', color = 'pink')\n",
    "plt.plot(np.arange(foo.size), foo, '--', label='Exactitud', color='red')\n",
    "plt.legend()\n",
    "plt.xlabel('Epocas')\n",
    "plt.ylabel('Perida / Exactitud')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
