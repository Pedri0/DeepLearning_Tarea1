{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primero la teoria\n",
    "\n",
    "### a) Ecuaciones de retropropagacion\n",
    "\n",
    "La operación llevada acabo por la neurona artificial es una suma pesada evaluada en la funcion de activación $\\phi$. La suma pesada consiste en multiplicar cada entrada por su correspondiente peso y sumar el sesgo, es decir,\n",
    "\n",
    "$z= w_1 \\cdot x_1 + w_2 \\cdot x_2 + ... + w_n \\cdot x_n + b$\n",
    "\n",
    "o bien,\n",
    "\n",
    "$z = W^{T}\\vec{x} + b$\n",
    "\n",
    "Para una red neuronal densa de dos capas ocultas tenemos un dibujo algo asi:\n",
    "\n",
    "E -> O1 -> O2 -> S\n",
    "\n",
    "donde E es la capa de entrada, O1 es la capa oculta 1,  O2 es la capa oculta 2 y S es la capa de salida.\n",
    "\n",
    "La propagacion hacia adelante depende de la red densa. Como para este caso tenemos dos capas ocultas y una de salida vamos a tener tres matrices de pesos $W_{1,2,3}$ y tres sesgos $b_{1,2,3}$ una para cada una de las capas. Para este caso la propagacion hacia adelante se define como:\n",
    "\n",
    "$$\\vec{a}^{(1)} = \\vec{x}^{(i)}$$\n",
    "$$\\vec{z}^{(2)} = W^{(1)T} \\cdot \\vec{a}^{(1)} + \\vec{b}^{(1)}$$\n",
    "$$\\vec{a}^{(2)} = \\phi (\\vec{z}^{(2)})$$\n",
    "$$\\vec{z}^{(3)} = W^{(2)T} \\cdot \\vec{a}^{(2)} + \\vec{b}^{(2)}$$\n",
    "$$\\vec{a}^{(3)} = \\phi (\\vec{z}^{(3)})$$\n",
    "$$\\vec{z}^{(4)} = W^{(3)T} \\cdot \\vec{a}^{(3)} + \\vec{b}^{(3)}$$\n",
    "$$\\vec{a}^{(4)} = \\phi (\\vec{z}^{(4)})$$\n",
    "$$\\hat{y}^{(i)} = a^{(4)}$$\n",
    "\n",
    "donde $\\phi$ es la funcion de activacion,  $\\vec{z}^{(i)}$ puede considerarse como el vector de entrada a una neurona, mientras que $\\vec{a}^{(i)}$ lo interpretamos como la respuesta de la neurona\n",
    "\n",
    "El algoritmo de retropropagacion nos dice que: Debemos calcular las derivadas parciales de la funcion de costo $C$ con respecto a cada peso y sesgo capa por capa, comenzando por la capa de salida y propogandolas hacia atras para calcular las de la capa anterior.\n",
    "\n",
    "Para explicar el algoritmo de retropropagacion formalmente es necesario introducir notacion formal:\n",
    "\n",
    "Sean:\n",
    "\n",
    "$w_{jk}^{l}$: el peso proveniente de la nuerona $k$ en la capa $l-1$ a la neurona $j$ en la capa $l$\n",
    "\n",
    "$b_{j}^{l}$: el sesgo de la neurona $j$ en la capa $l$\n",
    "\n",
    "$a_{j}^{l}$: la funcion de activacion de la neurona $j$ en la capa $l$\n",
    "\n",
    "con esta notacion tenemos que: $a_{j}^{l}=\\phi \\left( \\sum_{k} w^{l}_{jk}a^{l-1}_{k} + b^{l}_{j}   \\right)$ donde la suma se realiza sobre todas las k neuronas en la capa $l-1$. Para reescribir la ecuacion anterior de forma mas sencilla definimos $w^{l}$ como una matriz que contienen los pesos qie conectas a las neuronas de la capa $l$. De forma analoga definimos el vector $b^{l}$ para cada capa $l$. Asi tenemos que $a^{l} = \\phi (w^{l}a^{l-1}+b^{l})$. Para calcular a^{l} es necesario calcular $z^{l}$ que se define como $z^{l} = w^{l}a^{l-1}+b^{l}$\n",
    "\n",
    "La meta de la retroporpagacion consiste en calcular las derivadas parciales $\\partial C$ $/$ $\\partial w$ y $\\partial C$ $/$ $\\partial b$ de la funcion de costo $C$ con respecto a todos los pesos $w$ o sesgos $b$ en la red. Para que el algoritmo de retropropagacion funcione o este bien definida es necesario realizar dos suposiciones principales acerca de la funcion de costo $C$. Estas suposiciones son:\n",
    "* La funcion de costo puede ser escrita como un promedio $C = \\frac{1}{n}\\sum_{x}C_{x}$ sobre las funciones de costo $C_{x}$ para ejemplos individuales de entrenamiento. Esto es porque la retropropagacion nos permite cal cular las derivadas parciales $\\partial C_x$ $/$ $\\partial w$ y $\\partial C_x$ $/$ $\\partial b$ para un ejemplo de entrenamiento y entonces es posible obtener $\\partial C$ $/$ $\\partial w$ y $\\partial C$ $/$ $\\partial b$ al promediar sobre todos los ejemplos de entrenamiento\n",
    "* La funcion de costo puede ser escrita como una funcion de las salidas de la red neuronal, es decir Costo $C = C(a^{L})$, donde $a^{L}$ es el vector de activaciones de salida.\n",
    "\n",
    "LA retropropagacion consiste en entender como cambia la funcion de costo al cambiar los pesos y los sesgos de la real, es decir calcular las derivadas parciales. Para calcularlas es necesario introducir una nueva cantidad que denotamos como $\\delta_{j}^{l}$ y es el error en la neurona $j$ en la capa $l$ y la definimos como $$\\delta_{j}^{l} = \\frac{\\partial C}{\\partial z^{l}_{j}}$$. Tambien podemos definir $\\delta^{l}$ como el vector de errores asociados en la capa $l$\n",
    "\n",
    "A partir de esta ultima expresion es posible derivar las ecuaciones de la retropropagacion. Esto se hace a continuacion:\n",
    "\n",
    "1) $\\delta_{j}^{L} = \\frac{\\partial C}{\\partial z^{L}_{j}}$ corresponde al error de salida o el error de la capa de salida. Aplicando la regla de la cadena tenemos que: $$\\delta_{j}^{L} = \\sum_{k}\\frac{\\partial C}{\\partial{a_{k}^{L}}}\\frac{\\partial{a_{k}^{L}}}{\\partial z^{L}_{j}}$$\n",
    "Donde la suma es sobre todas las k neuronas de la ultima capa. Dado que la funcion de activacion $a^{L}_{k}$ depende unicamente de $z^{L}_{j}$ para la neurona $j$, entonces $\\partial a^{L}_k$ $/$ $\\partial z^{L}_{j}$ es cero si y solo si j es diferente de k. Luego tenemos que: $$\\delta_{j}^{L} = \\frac{\\partial C}{\\partial{a_{j}^{L}}}\\frac{\\partial{a_{j}^{L}}}{\\partial z^{L}_{j}}$$ que reescribimos como  $$\\delta_{j}^{L} = \\frac{\\partial C}{\\partial{a_{j}^{L}}}\\phi '(z_j^{L})$$\n",
    "donde $\\phi'(z_j^{L})$ es la derivada de la funcion de activacion con respecto a $z_j^{L}$\n",
    "\n",
    "2) Tenemos que $$\\delta_{j}^{l} = \\frac{\\partial C}{\\partial z^{l}_{j}} = \\sum_{k}\\frac{\\partial C}{\\partial{z_{k}^{l+1}}}\\frac{\\partial{z_{k}^{l+1}}}{\\partial z^{l}_{j}} = \\sum_{k}\\delta^{l+1}_{k}\\frac{\\partial{z_{k}^{l+1}}}{\\partial z^{l}_{j}}$$ La utima delta proviene de la definicion.\n",
    "Veamos que por definicion tenemos que $$z_{k}^{l+1} = \\sum_{j}w^{l+1}_{kj}a_{j}^{l} + b^{l+1}_k = \\sum_{j}w^{l+1}_{kj}\\phi(z^{l}_{j}) + b^{l+1}_k $$ \n",
    "Derivando con respecto a $z^{l}{j}$ tenemos\n",
    "$$\\frac{\\partial{z_{k}^{l+1}}}{\\partial z^{l}_{j}} = w^{l+1}_{kj} \\phi '(z_j^{l})$$ Sustituyendo se sigue que $$\\delta_{j}^{l} = \\sum_{k}w^{l+1}_{kj} \\phi '(z_j^{l}) \\delta^{l+1}_{k}$$\n",
    "\n",
    "3) Para calcular la derivada de C con respecto a los pesos W, tenemos que $$\\frac{\\partial C}{\\partial w^{l}_{jk}} = \\frac{\\partial C}{\\partial z_{j}^{l}}\\frac{\\partial z_{j}^{l}}{\\partial w^{l}_{jk}} = \\delta_{j}^{l}\\frac{\\partial z_{jk}^{l}}{\\partial w^{l}_{jk}}$$ dado que $$z_{jk}^{l} = \\sum_{k}w^{l}_{jk}a_{k}^{l-1} + b^{l}_j $$ Derivando con respecto a $w^{l}_{jk}$\n",
    "$$\\frac{\\partial{z_{k}^{l}}}{\\partial w^{l}_{jk}} = a^{l-1}_{k}$$ De esta forma se sigue que $$\\frac{\\partial C}{\\partial w^{l}_{jk}} = a^{l-1}_{k} \\delta_{j}^{l}$$\n",
    "\n",
    "4) Para calcular la derivada de C con respecto a los pesos W, tenemos que $$\\frac{\\partial C}{\\partial b^{l}_{j}} = \\frac{\\partial C}{\\partial z_{jk}^{l}}\\frac{\\partial z_{jk}^{l}}{\\partial b^{l}_{j}} = \\delta_{j}^{l}\\frac{\\partial z_{jk}^{l}}{\\partial b^{l}_{j}}$$ Para calcular la derivada usamos la definicion de $z_{jk}^{l}$ tenemos que $$\\frac{\\partial{z_{jk}^{l}}}{\\partial b^{l}_{j}} = 1$$ Luego: $$\\frac{\\partial C}{\\partial b^{l}_{j}} = \\delta_{j}^{l}$$\n",
    "\n",
    "\n",
    "De una forma mas compacta podemos escribir las ecuaciones de retropropagacion como sigue:\n",
    "1) $$\\delta ^{L} = \\vec{\\nabla}_a C \\odot \\phi '(z^{L}) $$ donde $\\odot$ denota el producto de Hadamard\n",
    "2) $$\\delta^{l} = \\left[ \\left(w^{(l+1)} \\right)^{T}\\delta^{l+1} \\odot \\phi '(z^{l})  \\right]$$\n",
    "3) $$\\frac{\\partial C}{\\partial b^{l}_{j}} = \\delta_{j}^{l}$$\n",
    "4) $$\\frac{\\partial C}{\\partial w^{l}_{jk}} = a^{l-1}_{k} \\delta_{j}^{l}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo de retropropagacion\n",
    "\n",
    "Una vez que conocemos las ecuaciones de retropropagacion podemos dar un bozquejo del algoritmo de retropropagacion, que nos permite calcular el gradiente de una funcion de costo $C$. El algoritmo es como sigue:\n",
    "1) Entrada $\\vec{x}$: Establece la activacion $a^{(1)}$ correspondiente a la capa de entrada\n",
    "\n",
    "2) Propagacion hacia adelante: para cada capa $l = 2,3,4,...,L$ calcula $z^{l}=w^{l}a^{l-1}+b^{l}$ y $a^{l}=\\phi(z^{l})$\n",
    "\n",
    "3) Calcula el error de salida $\\delta^{L} = \\vec{\\nabla}_a C \\odot \\phi '(z^{L})$\n",
    "\n",
    "4) Retropropaga el error: Para cada $l = L-1,L-2,...,2$ calcula $\\delta^{l} = \\left[ \\left(w^{(l+1)} \\right)^{T}\\delta^{l+1} \\odot \\phi '(z^{l})  \\right]$\n",
    "\n",
    "5) Salida: el gradiente de la funcion de costo $C$ esta dado por $\\frac{\\partial C}{\\partial b^{l}_{j}} = \\delta_{j}^{l}$ y $\\frac{\\partial C}{\\partial w^{l}_{jk}} = a^{l-1}_{k} \\delta_{j}^{l}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El algoritmo del descenso del gradiente\n",
    "\n",
    "En las redes neuronales y en aprendizaje de maquina uno de los objetivos mas importantes del entrenamiento consiste en buscar que la funcion de costo $C$ presente valores minimos para todos los ejemplos de entrenamiento. Dado que en una red densa ya conocemos una manera de calcular el gradiente de dicha funcion podemos usar las herramientas de calculo para conseguir que $C$ sea minimo, esto se realiza al conseguir que el gradiente de $C$ con respecto a los pesos y los sesgos sea igual a cero, aqui suponemos que la funcion $C$ es pseudo convexa. Dado que el gradiente de $C$ evaluado en un punto cualquiera se interpreta como la pendiente de la recta, el algoritmo del decenso del gradiente propone que los pesos y los sesgos se actualicen de tal forma que a cada iteracion estos se acerquen a un minimo global o local de acuerdo a la evaluacion de la funcion $C$ en dichos puntos. La actualizacion esta definida como el incremento $$\\Delta w^i = -\\gamma \\frac{\\partial C}{\\partial w^{i}}$$ y $$\\Delta b^i = -\\gamma \\frac{\\partial C}{\\partial b^{i}}$$ Donde $\\gamma$ es un parametro libre o hiperparametro conocido como tasa de aprendizaje.\n",
    "\n",
    "De esta forma la actualizacion de los pesos y sesgos en el algoritmo del descenso del gradiente es iterativo y para una iteracion o tiempo $t$ tenemos que la regla de actualizacion esta dada por $$\\Gamma(t+1) = \\Gamma(t) - \\gamma \\vec{\\nabla}\\Gamma(t)$$ donde $\\Gamma = (\\vec{w}, \\vec{b})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso particular entropia cruzada binaria\n",
    "\n",
    "La funcion de costo de la entropia cruzada binaria es $$C(\\vec{y},\\hat{\\vec{y}}) = -\\sum^{N}_{i=1} \\left(y^{(i)}log\\hat{y}^{(i)} + (1-y^{(i)})log(1-\\hat{y}^{(i)})  \\right)$$ Calculemos la derivada de $C$ con respecto a los pesos es decir $$\\frac{\\partial C}{\\partial w_j} = -\\sum_{x} \\left( \\frac{y}{\\sigma(z)} - \\frac{(1-y)}{1-\\sigma(z)}\\right)\\sigma ' (z)x_j$$ ya que $\\hat{y}^{(i)} = a^{(4)} = \\phi (\\vec{z}^{(4)})$ y para este caso $\\phi$ es la funcion sigmoide $\\sigma(z)$. Ahora la derivada con respecto a los sesgos tenemos que $$\\frac{\\partial C}{\\partial b} = -\\sum_{x} \\left( y(1-\\sigma(z)) + (y-1)\\sigma(z)\\right) = \\sum_{x} \\sigma(z)-y$$ Esto para el caso de una sola neurona. Para multiples neuronas tenemos que la funcion de costo podemos expresarla como $$C(\\vec{y},\\hat{\\vec{y}}) = -\\sum_{x}\\sum_{j} \\left(y_{j}log\\hat{y}_{j} + (1-y_{j})log(1-\\hat{y}_{j})  \\right)$$ donde $x$ representa las entradas y $j$ son las neuronas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
