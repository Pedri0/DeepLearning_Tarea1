{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos las librerias utiles\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1: Red de unidades de umbral lineal\n",
    "\n",
    "Programa y evalua una red de neuronas con funciones de activacion escalon unitario que aproxime la operacion XNOR dada por:\n",
    "\n",
    "\n",
    "| $x_1$ | $x_2$ | $y$ |\n",
    "| :- | :- | :- |\n",
    "| 0 | 0 | 1 \n",
    "| 0 | 1 | 0\n",
    "| 1 | 0 | 0\n",
    "| 1 | 1 | 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de activación escalon está dada por \n",
    "\n",
    "$\\phi=   \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      1 & x> 0 \\\\\n",
    "      0 & otro caso \\\\\n",
    "\\end{array} \n",
    "\\right.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creamos una funcion en python para representar la funcion de activacion\n",
    "#escalon\n",
    "def escalon(z):\n",
    "    if z > 0.0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La operación llevada acabo por la neurona artificial es una suma pesada evaluada en la funcion de activación $\\phi$. La suma pesada consiste en multiplicar cada entrada por su correspondiente peso y sumar el sesgo, es decir,\n",
    "\n",
    "$z= w_1 \\cdot x_1 + w_2 \\cdot x_2 + ... + w_n \\cdot x_n + b$\n",
    "\n",
    "o bien,\n",
    "\n",
    "$z = W^{T}\\vec{x} + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creamos en python una funcion que haga la suma pesada\n",
    "def neurona(x,w,b):\n",
    "    z = np.dot(w.T,x) + b\n",
    "    activacion = escalon(z)\n",
    "    return activacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder modelenar una red que aproxime un XNOR escribimos el algoritmo del perceptron multicapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#algoritmo del perceptron multicapa\n",
    "def perceptron(X,y,epocas):\n",
    "    #inicializamos la matriz de pesos y el sesgo en cero\n",
    "    W = np.zeros(X.shape[1])\n",
    "    b = 0\n",
    "    #iteramos sobre las epocas, i.e, hacemos un paso a la vez hasta\n",
    "    #cumplir el valor de epocas\n",
    "    for i in range(epocas):\n",
    "        #definimos variable auxiliar para llevar la marca del error\n",
    "        #en cada epoca\n",
    "        errorActual = 0.0\n",
    "        #iteramos sobre todos los ejemplos\n",
    "        for j in range(X.shape[0]):\n",
    "            #definimos variables auxiliares que guardan los valores\n",
    "            #de W y b en una epoca anterior\n",
    "            wAnterior = W\n",
    "            bAnterior = b\n",
    "            #realizamos la prediccion usando la funcion neurona\n",
    "            yHat = neurona(X[j],wAnterior,bAnterior)\n",
    "            #calculamos el error de prediccion\n",
    "            error = y[j] - yHat\n",
    "            #Algoritmo del descenso del gradiente para actualizar\n",
    "            #los pesos y el sesgo\n",
    "            W = wAnterior + error*X[j]\n",
    "            b = bAnterior + error\n",
    "            #acumulamos el error actual provisto por cada ejemplo\n",
    "            errorActual += np.abs(error)\n",
    "        #imprimimos el error de una epoca\n",
    "        print(\"Para la epoca {0} hay un error de {1}\".format(\n",
    "        i,errorActual/float(X.shape[0])))\n",
    "    return W , b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para la epoca 0 hay un error de 0.25\n",
      "Para la epoca 1 hay un error de 0.5\n",
      "\n",
      "w_1 = 1.0, w_2 = 1.0, b = 1.0\n",
      "-----------------------------\n",
      "x_1 \tx_2 \t y\ty_hat\n",
      "-----------------------------\n",
      "0.0\t0.0\t0.0\t1.0\n",
      "0.0\t1.0\t1.0\t1.0\n",
      "1.0\t0.0\t1.0\t1.0\n",
      "1.0\t1.0\t1.0\t1.0\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])\n",
    "y_or = np.array([0., 1., 1., 1.]) \n",
    "\n",
    "w, b = perceptron(X, y_or,2)\n",
    "\n",
    "print('\\nw_1 = {0}, w_2 = {1}, b = {2}'.format(w[0], w[1], b))\n",
    "print('-----------------------------')\n",
    "print('x_1 \\tx_2 \\t y\\ty_hat')\n",
    "print('-----------------------------')\n",
    "for i in range(X.shape[0]):\n",
    "  y_hat = neurona(X[i], w, b)\n",
    "  print('{0}\\t{1}\\t{2}\\t{3}'.format(X[i, 0], X[i, 1], y_or[i], y_hat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
